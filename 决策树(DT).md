#决策树（DT）

##算法流程：

在决策树的每一个节点上面，如果有没有进行划分的属性或者实例并不是都在属性上取值相同，或者都属于同一类，那么寻找一个最佳的划分属性。

寻找最佳划分属性是主要的任务。

##划分选择：

###信息增益：

####信息熵：

度量样本集合纯度的一个重要指标。越小越好。

####信息增益：

以属性a作为划分的依据，在这个属性上取值不同的各个样本集合分别计算信息熵，按照各个样本集合占总集合的比例加权求和为划分后的总信息熵。用没有进行划分的信息熵减去划分后的信息熵为信息增益。越大越好。

信息增益会偏向于取值多的属性，每个取值的纯度会相对比较高。

####信息增益率：

在每个属性的信息增益基础上除以属性的“固有值”。

信息增益率会相对偏向于取值少的属性。

###基尼指数：

Gini指数反应的是随机从数据集中抽取两个样本，其所属类别不同的比例。

对于每个划分属性，对其所有取值的集合计算基尼指数，最小的作为最佳划分属性。

##重要方法：

###ID3

使用信息增益作为最佳属性划分的选择依据。

###C4.5

1、启发式的在信息增益高于平均值的属性里面，挑选信息增益率最大的属性作为最佳划分属性。

2、连续值处理中使用“二分法”

3、缺失值处理中使用按照比例进行，某个属性上缺失就按照比例将样本划分到不同取值集合

###CART

使用基尼指数作为划分依据。

##剪枝处理：

###作用：

防止过拟合的有效手段。

###预剪枝：

在每一个节点都判断划分前后的泛化能力，如果变强继续划分，否则禁止在这一节点划分。

####优点：

1、降低了过拟合风险；

2、有效降低了训练时间开销

####不足：

贪心的判断，有可能造成欠拟合。即有些分支虽然当前不能提高泛化能力，但是在此基础上的后续划分有可能会提高。

###后剪枝：

在整个决策树生成之后，自底向上判断每个非叶节点是否应该划分。无法提高泛化能力就不划分，否则划分。

####优点：

欠拟合风险很小，泛化能力一般要优于预剪枝

####不足：

计算复杂度远大于预剪枝

##连续值处理:

对连续取值离散化，使用“二分法”。即对于每一个属性，在样例中找到所有样本在该属性上不同取值的中间值，作为划分。计算不同划分的信息增益，取最高的作为划分点。

需要注意的是连续属性还可作为其后代节点的划分属性，只要划分节点不同即可。

##缺失值处理：

###如何找最佳划分属性：

最佳属性按照样本中在这个属性上没有缺失的样本进行计算，最后计算信息增益的时候乘以相应的比例即可。

###对于某个属性如何对缺失此属性值的样本进行划分：

将这个样本按照比例分配到各个分支节点当中去。

##多变量决策树：

相当于在每个节点上不是判断一个最佳的划分属性，而是得到一个最佳的属性线性组合

##优点：

* 训练时间复杂度较低，预测的过程比较快速；
* 模型容易展示（容易将得到的决策树做成图片展示出来）

##缺点：

单决策树容易过拟合，虽然有一些方法，如剪枝可以减少这种情况，但是还是不够的。