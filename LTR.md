#LTR

##总的介绍

分为三种方法：Pointwise，pairwise和listwise。

* **Pointwise**完全从单文档的分类角度计算，没有考虑文档之间的相对顺序。而且它假设相关度是查询无关的，只要（query，di）的相关度相同，那么他们就被划分到同一个级别中，属于同一类。然而实际上，相关度的相对性是和查询相关的，比如一个常见的查询它会有很多相关的文档，该查询和它相关性相对靠后的文档的label标注级别时可能会比一个稀有的查询和它为数不多的高度相关文档的label标准级别更高。这样就导致训练样本的不一致，并且对于预测为同一label级别的文档之间也无法相对排序。

* **Pairwise**考虑查询相关，将一个查询中label不同的文档对作为训练实例，（di,dj）如果di>dj则为正例，变为了二值分类问题。最后新的预测数据可以给每两个文档对预测分类，根据每两个的序列关系可以得到总的关系。
* 虽然考虑了两个文档的相对顺序，可是pairwise的方法没有考虑文档对在搜索结果中的排序位置。靠前的文档对比靠后的重要性高，所以排序错误的惩罚权重应该设置的更大；
* 如果不同查询的结果不平衡，那么准确率的评估会偏向结果多的那一方。

* **Listwise** 是将统一查询的所有结果作为一个序列实例进行学习。根据训练样本学习得到最优打分函数，然后根据打分函数对于每个文档的打分结果进行排序，得到最终的结果。最优打分函数是要对所有结果的各种排序结果进行打分，其分布与目标打分函数的结果最接近为最优打分函数。

##Pointwise

转化为普通的分类回归问题解决。

##Pairwise

###RankNet


由微软研究院的Chris Burges等人在2005年ICML上的一篇论文 Learning to Rank Using Gradient Descent 中提出，并被应用在微软的搜索引擎Bing当中。

####基本思想:

以交叉熵为损失函数（容易利用梯度下降法进行求解），学习一些模型来计算pair对顺序的概率，进而求得所有文档对的关系和总的排序关系。

####流程：

首先需要一个打分的函数f(x)

然后需要一个预测概率pij和一个实际的概率Pij，他们都是用f(x)定义的

接着用两个概率的交叉熵作为损失函数

对损失函数求打分函数的权重W的偏导，并进行梯度下降计算，得到打分函数。

####改进算法：

类似于mini-batch的改进，不是每次调整一对进行参数的传递，而是进行有关于ui的所有文档对，再进行参数传递调整。

####优势：

使用的是交叉熵作为损失函数，它求导方便，适合梯度下降法的框架；而且，即使两个不相关的文档的得分相同时，C也不为零，还是会有惩罚项的。

##Listwise

###LambdaRank

####对RankNet的改进

LambdaRank是一个经验算法，它直接定义的了损失函数的梯度λ，也就是Lambda梯度。Lambda梯度由两部分相乘得到：(1)RankNet中交叉熵概率损失函数的梯度；(2)交换Ui，Uj位置后IR评价指标Z的差值。

损失函数的梯度代表了文档下一次迭代优化的方向和强度，由于引入了IR评价指标，Lambda梯度更关注位置靠前的优质文档的排序位置的提升。有效的避免了下调位置靠前优质文档的位置这种情况的发生。
LambdaRank相比RankNet的优势在于考虑了评价指标，直接对问题求解，所以效果更好。

###LambdaMART

lambdaMART相当于在GBDT中使用lamda梯度，加入了排问题评价指标的信息，对排序问题直接求解。

##参考

http://blog.csdn.net/nanjunxiao/article/details/8976195

http://x-algo.cn/index.php/2016/07/31/ranknet-algorithm-principle-and-realization/

http://blog.csdn.net/puqutogether/article/details/42124491

http://www.jianshu.com/p/3c5c1ea7d836

http://x-algo.cn/index.php/2016/08/16/1026/

http://liam0205.me/2016/07/10/a-not-so-simple-introduction-to-lambdamart/